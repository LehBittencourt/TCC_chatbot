import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import MessagesPlaceholder

from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

import torch
from langchain_huggingface import ChatHuggingFace
from langchain_community.llms import HuggingFaceHub


from dotenv import load_dotenv

load_dotenv()

# Configura√ß√µes do Streamlit
st.set_page_config(page_title="Seu assistente virtual ü§ñ", page_icon="ü§ñ")
st.title("Seu assistente virtual ü§ñ")

model_class = "hf_hub" # @param ["hf_hub", "openai", "ollama"]

# Fun√ß√£o de Carregamento dos Modelos

def model_hf_hub(model = "meta-llama/Meta-Llama-3-8B-Instruct", temperature=0.1):
  llm = HuggingFaceHub(repo_id=model,
                       model_kwargs={
                          "temperature":temperature,
                          "return_full_text": False,
                          "max_new_tokens": 512,  # n¬∞ max de tokens gerados
                          })
  return llm

def model_openai(model = "gpt-4o-mini", temperature=0.1):
  llm = ChatOpenAI(model= model, temperature=temperature)
  return llm

def model_ollama(model = "phi3", temperature=0.1):
  llm = ChatOllama(model=model, temperature=temperature)
  return llm

# Fun√ß√£o para fazer conversa com chatbot

def model_response(user_query, chat_history, model_class):  # Usu√°rio solicitar, hist√≥rico das conversas,

    # Carregamento da LLM
    if model_class == "hf_hub":
      llm = model_hf_hub()
    elif model_class == "openai":
      llm = model_openai()
    elif model_class == "ollama":
      llm = model_ollama()

    # Defini√ß√£o do Prompt
    system_prompt = """
        Voc√™ √© um assistente prestativo e est√° respondendo perguntas gerais. Responda em portugu√™s."
    """

    # Adequando Pipeline
    if model_class.startswith("hf"):
        user_prompt = "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>" # modelo padr√£o do hugfface que recebe o input
    else:
        user_prompt = "{input}" # Valor padr√£o para outros modelos

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt), # Recebe o prompt
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", user_prompt)      # Recebe a pergunta do usu√°rio
    ])

    # Cria√ß√£o da chain
    # StrOutputParser(): faz a formata√ß√£o final do prompt

    chain = prompt_template | llm | StrOutputParser()

    # Retorno da resposta
    return chain.stream({"chat_history": chat_history,"input": user_query})

    # chain.invoke(): retorna a resposta quando todo processamento for conclu√≠do
    # chain.stream(): o texto ser√° mostrado a medida que √© gerado

# Controla as mensagens trocadas com o chatbot, pois depois de um tempo sao apagadas do chache se nao usar o chatbot

if "chat_history" not in st.session_state: # Verifica se a vari√°vel chat_history tem algum valor. se sim a sess√£o est√° ativa e j√° estamos nos comunicando com chatbot
    st.session_state.chat_history = [      # Se n√£o vamos criar outra cessao e guando abrir o site o chat vai da uma mensagem de boas vindas
        AIMessage(content="Ol√°, sou o seu assistente virtual! Como posso ajudar voc√™?"),
    ]

# Defini√ß√£o da conversa (renderiza√ß√£o do hist√≥rico de mensagens)
for message in st.session_state.chat_history: # Acessa o hist√≥rico de mensagens, nela fica armazenadas as mensagens do usu√°rio e da IA
    if isinstance(message, AIMessage): # Verifica se a mesagem √© um objeto da classe IAMessage, todas elas v√£o ser respostas da IA
        with st.chat_message("AI"):
            st.write(message.content) # Escreve as mesagens na tela
    elif isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.write(message.content)

user_query = st.chat_input("Digite sua mensagem aqui...")
if user_query is not None and user_query != "":
    st.session_state.chat_history.append(HumanMessage(content=user_query))

    with st.chat_message("Human"):
        st.markdown(user_query)   # Formata exibe a msg na interface do st

    with st.chat_message("AI"):
        resp = st.write_stream(model_response(user_query, st.session_state.chat_history, model_class))
        print(st.session_state.chat_history)

    st.session_state.chat_history.append(AIMessage(content=resp))
